{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing \n",
    "\n",
    "Date: 02/02/2020\n",
    "\n",
    "Version: 5\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here:\n",
    "* BeautifulSoup (for parse XML file, included in Anaconda Python 2.7) \n",
    "* re (for regular expression, included in Anaconda Python 2.7) \n",
    "* nltk.corpora (access the corpora in the NLTK data package, included in Anaconda Python 2.7) \n",
    "* nltk.stem (for removing morphological affixes from words, included in Anaconda Python 2.7) \n",
    "* nltk.tokenize (for dividing a string into substrings, included in Anaconda Python 2.7) \n",
    "* nltk.probality (for the finding and ranking of quadgram collocations or other association measures, included in Anaconda Python 2.7)\n",
    "* nltk.util (Return the bigrams generated from a sequence of items, included in Anaconda Python 2.7) \n",
    "* nltk.collocations (for the finding and ranking of bigram, included in Anaconda Python 2.7)\n",
    "* itertool.chain (for making an iterator, included in Anaconda Python 2.7)\n",
    "* CountVectorizer (for creating a document-term matrix, included in Anaconda Python 2.7)\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python file demonstrates how natural language processing is applied to a Patent XML file. The result can be used to do a statistical relational learning for a company in the future. Statistical relational learning can be found everywhere. For example, when an internet user search a book on Amazon website, the website not only will show this book, but also will recommend several books in the same kind. Therefore, in order to solve information overload problems or help users to search unspecified data, many companies take advantage of personalized recommendation systems to help their customers to find what they are really interested in. \n",
    "\n",
    "This original Patent file provides 2500 patents. Each patent has a large amount of information which can be analyzed. This task is going to extract Citation Networks, a Hierarchical IPC Codes, and Abstracts from each patant. This file shows how pre-processing is going to do and it can also give users relevant data in the future. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has 4 tasks. First is to extract \"Class\", \"Subclass\", \"Main Group\", and \"Subgroup\" from each patent. We can use either element tree or beautiful soup library. My concern is that if we use element tree to parse the given XML file, it cannot directly parse the file since it has searval roots, but beautifulsoup can easily make it possible by just using the interpreter 'html.parser'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 and 3 are very tricky since it is going to extract its own citation IDs from each patent. We all know that each patent may or may not be another patents' citation. And one patent usually has several citations. Applying \"find\" and \"findall\" function accurately will help us to finish this task efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4 is a much more challenge task and it is the most important part for preparing statistical relation learning. The general steps would be pre-processing (including tokenise), remove stopwords, create bigram and build count vector. After doing this task, I find there are several ways to do for this part, and finally I take advanatage of the better one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as needed in this assessment\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.probability import *\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import collocations\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from __future__ import division\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 2.7.16 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examining and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the file\n",
    "Here comes a chanllenge problem for parsing the XML file: only a part of XML file can be read not all of it. I think it is because\n",
    "every encode tag is at the beginning of the part of a file so that beautifulsoup cannot recognize several roots. I try several methods(such as just open the file and parse) since I want to know which method is more easy and straightforward. \n",
    "When I went through the beautifulsoup documentation in the official website, I notice that using Python’s \"html.parser\" instead of \"lxml\" function to read the file is much better to solve this problem, but the shortage is that the running time is slower than the \"lxml\" parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"./patents.xml\"),\"html.parser\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parsing XML and Extracting all the required information \n",
    "\n",
    "Here, you should write your own code to parsing the xml file, and extract the hierarchical IPC code, citation network, and abstracts. We strongly suggest that you use existing Python libraries to parse the xml file. Store the extracted information in files if required in the assessment description.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: Finding all patent ID\n",
    "Finding all patent ID by using beautifulsoup.\n",
    "Because there are many patent ID in each part, our goal is to find the doc-number under publication reference tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PP021722', u'RE042159', u'RE042170']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding all of publication-ference tag.\n",
    "application=soup.find_all(\"publication-reference\")\n",
    "#there is just 1 patentid under 'doc-number'tag so that we can just use 'find' instead of 'findall' function \n",
    "docid=[item.find(\"doc-number\") for item in application]\n",
    "#extract text contents\n",
    "patentid=[item.string for item in docid]\n",
    "patentid[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the total number of patent ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of patent ID:  2500\n"
     ]
    }
   ],
   "source": [
    "print \"the number of patent ID: \",len(patentid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: Finding all of corrsponding tags\n",
    "Finding all corresponding tags by using beautifulsoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting section,patent class,subclass, main-group and subgroup by using findall function\n",
    "section = [section.string for section in soup.find_all(\"section\")]\n",
    "patent_class = [pclass.string for pclass in soup.find_all(\"class\")]\n",
    "subclass = [subclass.string for subclass in soup.find_all(\"subclass\")]\n",
    "main_group = [main_group.string for main_group in soup.find_all(\"main-group\")]\n",
    "subgroup=[sub.string for sub in soup.find_all(\"subgroup\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Testing the length of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of section is:  2500 \n",
      "length of patent class is:  2500 \n",
      "length of subclass is:  2500 \n",
      "length of main group is:  2500 \n",
      "length of subgroup is:  2500\n"
     ]
    }
   ],
   "source": [
    "print \"length of section is: \", len(section),\"\\nlength of patent class is: \",len(patent_class),\\\n",
    "\"\\nlength of subclass is: \",len(subclass),\"\\nlength of main group is: \",len(main_group),\\\n",
    "\"\\nlength of subgroup is: \",len(subgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: writing to a text file\n",
    "Now we can see all of the elements have the same length. It's time to write to a file in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_handle=open('classification.txt','w+')\n",
    "for i in range(len(patentid)):\n",
    "    #make all of pattern to string\n",
    "    new_line=str(patentid[i])+\":\"+str(section[i])+\",\"+str(patent_class[i])+\",\"+str(subclass[i])+\",\"+str(main_group[i])+\",\"+str(subgroup[i])+\"\\n\"\n",
    "    output_handle.write(new_line)\n",
    "    i=i+1\n",
    "    \n",
    "output_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output file is under the same dicrectory with this jupiter file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: Finding all of citation \n",
    "In order to find all of citations, first of all is to get all of the references-cited tags. Then all of doc-numbers have been found under the referencs-cited tags because one references-cited tag has several doc-number tags. It will return a list withous moving any doc-number tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<doc-number>4954776</doc-number>,\n",
       " <doc-number>4956606</doc-number>,\n",
       " <doc-number>5015948</doc-number>,\n",
       " <doc-number>5115193</doc-number>,\n",
       " <doc-number>5180978</doc-number>,\n",
       " <doc-number>5332966</doc-number>,\n",
       " <doc-number>5332996</doc-number>,\n",
       " <doc-number>5351003</doc-number>,\n",
       " <doc-number>5381090</doc-number>,\n",
       " <doc-number>5521496</doc-number>,\n",
       " <doc-number>5914593</doc-number>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find all of reference-cited tag\n",
    "reference=soup.find_all(\"references-cited\")\n",
    "#because there are more than 1 doc-number under reference-cited, we still need to use findall function to search\n",
    "citation=[item.find_all(\"doc-number\") for item in reference]\n",
    "citation[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: Writing into a text file\n",
    "My method is to take cited IDs from the index of the patent IDs. For example: when I use the first patent ID, I can take the first element from citation list and extract all of the corresponding cited IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_handle=open('citations.txt','w+')\n",
    "variable=\"\"\n",
    "#let patent ID and its cited patent ID with the same index\n",
    "for i in range(len(patentid)):\n",
    "    for num in citation[i]:\n",
    "        for element in num:\n",
    "            #extract all of doc-numbers context contents\n",
    "            variable+=str(element.string)+\",\"\n",
    "    new_variable=variable.rstrip(\",\") #remove comma at the end of string\n",
    "    line=str(patentid[i])+\":\"+new_variable+\"\\n\"\n",
    "    variable=\"\"\n",
    "    output_handle.write(line)\n",
    "\n",
    "output_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of this question, I directly use the text file from the last question. My thoughts is that removing the citing IDs before comma, and then tokenise the comma between the cited IDs and the extra line tag which is between each lines. Then I use FreqDist function from \"nltk\" library to know the frquency of each cited IDs. Also, I sort the cited IDs to make users read and search more conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_handle=open('citations.txt','r')\n",
    "output_handle=open('cited.txt','w+')\n",
    "\n",
    "#make citation file as input file and read each line\n",
    "text_file=\"\"\n",
    "for line in input_handle:\n",
    "    for i in range(len(line)):\n",
    "        if line[i]==\":\":#remove the citing IDs before the colon\n",
    "            new_line=line[i+1:]\n",
    "            text_file+=new_line\n",
    "            \n",
    "#remove comma and extra line tag    \n",
    "tokenizer = RegexpTokenizer(r\"[\\n\\,]\", gaps=True) \n",
    "words=tokenizer.tokenize(text_file) \n",
    "\n",
    "#automatically create a dictionary with the key for cited IDs and value for frequency\n",
    "fdist1=FreqDist(words)\n",
    "\n",
    "#sort the keys to decending order to make the user read or search a file conveniently\n",
    "for key in sorted(fdist1.keys()):\n",
    "    lines=str(key)+\":\"+str(fdist1[key])+\"\\n\"\n",
    "    output_handle.write(lines)\n",
    "    \n",
    "input_handle.close()\n",
    "output_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "For this part of questions, my procedure is : \n",
    "* 1.extract abstracts\n",
    "* 2.tokenise \n",
    "* 3.stemming and Lemmatization\n",
    "* 4.remove stopwords \n",
    "* 5.bigram \n",
    "* 6.remove top-20 most frequent words \n",
    "* 7.remove words only appearing in one abstract\n",
    "* 8.bulid count vector \n",
    "* 9.write into a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### STEP 1: Extract all of abstracts\n",
    "Fiest of all, abstracts need to be extrated. From the XML file, every abstract may have 1 or more 'p' tags. It indicates that we need to find all of them under each patent ID. Then we convert list to string in order to prepare the next tokenise step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1: Finding all of abstract by using beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of abstract tags:  2500\n"
     ]
    }
   ],
   "source": [
    "#find all of  abstract tags\n",
    "abstract_tag=soup.find_all(\"abstract\")\n",
    "print \"The number of abstract tags: \",len(abstract_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2: Finding all of 'p' element under each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"p-0001\" num=\"0000\">A sensing device includes a circuit that compensates for time and spatial changes in temperature. The circuit includes elements to correct for variation in permeability of a highly permeable core of a differential variable reluctance transducer as temperature changes. The circuit also provides correction for temperature gradients across coils of the transducer.</p>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evry patentid may have serveral abstracts, finding al of 'p's under each patent ID.\n",
    "find_p=[item.find_all(\"p\") for item in abstract_tag]\n",
    "find_p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of 'p' elements:  2500\n"
     ]
    }
   ],
   "source": [
    "print \"The number of 'p' elements: \",len(find_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3: Extracting text contents between 'p' tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A sensing device includes a circuit that compensates for time and spatial changes in temperature. The circuit includes elements to correct for variation in permeability of a highly permeable core of a differential variable reluctance transducer as temperature changes. The circuit also provides correction for temperature gradients across coils of the transducer.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove 'p' tags and extract its text contents\n",
    "tag_list=[]\n",
    "for each in find_p:\n",
    "    a=[tag.string for tag in each]\n",
    "    tag_list.append(a)\n",
    "tag_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of text contents:  2500\n"
     ]
    }
   ],
   "source": [
    "print \"The number of text contents: \",len(tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4: merge those abstracts which have more than 2 text contents under a single abstract element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A new apple tree named \\u2018Daligris\\u2019 is disclosed. The fruit of the new variety is particularly notable for its eating quality and distinctive flavor and appearance. The fruit is very sweet and has a pronounced aniseed flavor, and takes on a distinctive red orange coloration as it ripens on the tree.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge those abstracts which have more than 2 text content from 'p's under the single abstract tag  \n",
    "new_tag_list=[]\n",
    "new_item=\"\"\n",
    "for each in tag_list:\n",
    "    #check if each item in a single patent ID has 2 or more abstracts\n",
    "    if len(each)<2:\n",
    "        new_tag_list.append(each)\n",
    "    else:\n",
    "        for item in each:\n",
    "            #some text content of p is \"\\n\"(extra line tag), it should be avoided\n",
    "            if item!=None:\n",
    "                new_item+=item\n",
    "        new_tag_list.append([new_item])\n",
    "        new_item=\"\"\n",
    "    \n",
    "new_tag_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of abstracts after merging 'p' tags:  2500\n"
     ]
    }
   ],
   "source": [
    "print \"The total number of abstracts after merging 'p' tags: \", len(new_tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5: Converting list to string in order to tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'A new apple tree named \\u2018Daligris\\u2019 is disclosed. The fruit of the new variety is particularly notable for its eating quality and distinctive flavor and appearance. The fruit is very sweet and has a pronounced aniseed flavor, and takes on a distinctive red orange coloration as it ripens on the tree.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#firstly: coverting inside list to string \n",
    "abstracts = [item for sublist in new_tag_list for item in sublist]\n",
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of abstracts:  2500\n"
     ]
    }
   ],
   "source": [
    "print \"number of abstracts: \",len(abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'a'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#secondly, converting list to string and use it to tokenise\n",
    "abstracts_text=\"\"\n",
    "for each in abstracts:\n",
    "    #case nomalisation\n",
    "    abstracts_text+=each.lower()\n",
    "abstracts_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing the abstracts \n",
    "\n",
    "In this section, you should write Python code to pre-process the abstracts, convert them into the required format, and store the processed data. We suggest that you refer to the lecture and tutorial materials and the references provided in those materials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Tokenise\n",
    "I already lower the case from last step, next, I am going to use regular expression to tokenise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenise \n",
    "pattern = r\"\"\"(?x)                   # set flag to allow verbose regexps\n",
    "              (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A.\n",
    "              |\\d+(?:\\.\\d+)?%?       # numbers, incl. currency and percentages\n",
    "              |\\w+(?:[-']\\w+)*       # words w/ optional internal hyphens/apostrophe\n",
    "              |(?:[+/\\-@&*])         # special characters with meanings\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a',\n",
       " u'new',\n",
       " u'apple',\n",
       " u'tree',\n",
       " u'named',\n",
       " u'daligris',\n",
       " u'is',\n",
       " u'disclosed',\n",
       " u'the',\n",
       " u'fruit',\n",
       " u'of',\n",
       " u'the',\n",
       " u'new',\n",
       " u'variety',\n",
       " u'is',\n",
       " u'particularly',\n",
       " u'notable',\n",
       " u'for',\n",
       " u'its',\n",
       " u'eating',\n",
       " u'quality',\n",
       " u'and',\n",
       " u'distinctive',\n",
       " u'flavor',\n",
       " u'and',\n",
       " u'appearance',\n",
       " u'the',\n",
       " u'fruit',\n",
       " u'is',\n",
       " u'very',\n",
       " u'sweet',\n",
       " u'and',\n",
       " u'has',\n",
       " u'a',\n",
       " u'pronounced',\n",
       " u'aniseed',\n",
       " u'flavor',\n",
       " u'and',\n",
       " u'takes',\n",
       " u'on',\n",
       " u'a',\n",
       " u'distinctive',\n",
       " u'red',\n",
       " u'orange',\n",
       " u'coloration',\n",
       " u'as',\n",
       " u'it',\n",
       " u'ripens',\n",
       " u'on',\n",
       " u'the',\n",
       " u'tree']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenised each abstract \n",
    "abstrcts_tokenised=[]\n",
    "for each in abstracts:\n",
    "    #case normalisation: make the context to lowercase\n",
    "    a=nltk.regexp_tokenize(each.lower(), pattern)\n",
    "    abstrcts_tokenised.append(a)\n",
    "abstrcts_tokenised[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3:  Stemming and Lemmatization\n",
    "After tokenising the whole text, it has a lot of similar words in different meanings. For example, nouns are inflected in the plural, verbs are inflected in the various tenses, and adjectives are inflected in the comparative/superlative. The derivation process creates a new word out of an existing one often by adding either a prefix or a suffix. Therefore, it is important to do stemming and lemmatization process, otherwise it would affect counting bigrams in the next step owing to tence or useless prefix(such as '/apple'). That's also the reason why I am doing lemmatization before doing the bigrams: it would help a lot with next several steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I tried 3 ways for stemming and accept the third one eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.The first method is to use the Porter Stemmer. However, for some special cases, the Porter Stemmer might not work as expected, like 'apple'→'appl','disclosed'→'disclos'. That're not we want. And bigram are used for speech recognition, spelling correction, entity detection, if we output wrong in advance from this step, the bigram does not make any sense either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'new', u'appl', u'tree', u'name', u'daligri', u'is', u'disclos', u'the', u'fruit', u'of', u'the', u'new', u'varieti', u'is', u'particularli', u'notabl', u'for', u'it', u'eat', u'qualiti', u'and', u'distinct', u'flavor', u'and', u'appear', u'the', u'fruit', u'is', u'veri', u'sweet', u'and', u'ha', u'a', u'pronounc', u'anise', u'flavor', u'and', u'take', u'on', u'a', u'distinct', u'red', u'orang', u'color', u'as', u'it', u'ripen', u'on', u'the', u'tree']\n"
     ]
    }
   ],
   "source": [
    "# stemmer_noun = PorterStemmer()\n",
    "# stemmer_list = []\n",
    "# ste_temp_list=[]\n",
    "# for each in abstrcts_tokenised:\n",
    "#     for item in each:\n",
    "#         stem_sent = stemmer_noun.stem(item)\n",
    "#         ste_temp_list.append(stem_sent)\n",
    "#     stemmer_list.append(ste_temp_list)\n",
    "#     ste_temp_list=[]\n",
    "# #show only first abstract text content\n",
    "# print stemmer_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.The Second method is to use the Porter Stemmer. However, for some special cases, the Porter Stemmer might not work as expected, like 'tree'→'tre','particular'→'particul'. That would influence with searching bigram as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'new', u'appl', u'tre', u'nam', u'daligr', u'is', u'disclos', u'the', u'fruit', u'of', u'the', u'new', u'vary', u'is', u'particul', u'not', u'for', u'it', u'eat', u'qual', u'and', u'distinct', u'flav', u'and', u'appear', u'the', u'fruit', u'is', u'very', u'sweet', u'and', u'has', u'a', u'pronount', u'anisee', u'flav', u'and', u'tak', u'on', u'a', u'distinct', u'red', u'orang', u'col', u'as', u'it', u'rip', u'on', u'the', u'tre']\n"
     ]
    }
   ],
   "source": [
    "# stemmer_verb = LancasterStemmer()\n",
    "# stemmer_list = []\n",
    "# ste_temp_list=[]\n",
    "# for each in abstrcts_tokenised:\n",
    "#     for item in each:\n",
    "#         stem_sent = stemmer_verb.stem(item)\n",
    "#         ste_temp_list.append(stem_sent)\n",
    "#     stemmer_list.append(ste_temp_list)\n",
    "#     ste_temp_list=[]\n",
    "# print stemmer_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.The third way is to use a lemmatizer that utilises more information about the language to accurately identify the lemma for each word. And looks much better. I accept this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'a', 'DT'), (u'new', 'JJ'), (u'apple', 'NN'), (u'tree', 'NN'), (u'named', 'VBN'), (u'daligris', 'NN'), (u'is', 'VBZ'), (u'disclosed', 'VBN'), (u'the', 'DT'), (u'fruit', 'NN'), (u'of', 'IN'), (u'the', 'DT'), (u'new', 'JJ'), (u'variety', 'NN'), (u'is', 'VBZ'), (u'particularly', 'RB'), (u'notable', 'JJ'), (u'for', 'IN'), (u'its', 'PRP$'), (u'eating', 'JJ'), (u'quality', 'NN'), (u'and', 'CC'), (u'distinctive', 'JJ'), (u'flavor', 'NN'), (u'and', 'CC'), (u'appearance', 'VB'), (u'the', 'DT'), (u'fruit', 'NN'), (u'is', 'VBZ'), (u'very', 'RB'), (u'sweet', 'JJ'), (u'and', 'CC'), (u'has', 'VBZ'), (u'a', 'DT'), (u'pronounced', 'VBN'), (u'aniseed', 'NN'), (u'flavor', 'NN'), (u'and', 'CC'), (u'takes', 'VBZ'), (u'on', 'IN'), (u'a', 'DT'), (u'distinctive', 'JJ'), (u'red', 'JJ'), (u'orange', 'NN'), (u'coloration', 'NN'), (u'as', 'IN'), (u'it', 'PRP'), (u'ripens', 'VBZ'), (u'on', 'IN'), (u'the', 'DT'), (u'tree', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#firstly, code should make use the POS tags of each word to decide the lexical base form\n",
    "tagged_sents = []\n",
    "for sent in abstrcts_tokenised:\n",
    "    tagged_sent = nltk.tag.pos_tag(sent)\n",
    "    tagged_sents.append(tagged_sent)\n",
    "#because the file is large, I just ask to show the first abstract to check the output\n",
    "print tagged_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    #tag startwith J,change to ADJ\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    #tag name startwith V, change to VERB\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    #tag name startwith N, change to NUUN\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    #tag name startwith R, change to ADV\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        #change else tag to NOUN\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'a', u'new', u'apple', u'tree', u'name', u'daligris', u'be', u'disclose', u'the', u'fruit', u'of', u'the', u'new', u'variety', u'be', u'particularly', u'notable', u'for', u'it', u'eating', u'quality', u'and', u'distinctive', u'flavor', u'and', u'appearance', u'the', u'fruit', u'be', u'very', u'sweet', u'and', u'have', u'a', u'pronounce', u'aniseed', u'flavor', u'and', u'take', u'on', u'a', u'distinctive', u'red', u'orange', u'coloration', u'a', u'it', u'ripen', u'on', u'the', u'tree']\n"
     ]
    }
   ],
   "source": [
    "#use wordnetlemmatizer to lemmization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "final_tokens =[]\n",
    "for tagged_set in tagged_sents:\n",
    "    final_tokens.append([lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) for w in tagged_set])\n",
    "#just show the first abstract\n",
    "print final_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now it is time to make a dictionary\n",
    "Each patent ID belongs to its own abstract/abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create dictionary\n",
    "dic={}\n",
    "#key values are patent id, values are corresponding abstracts after tokenise and stemming\n",
    "for i in range(len(patentid)):\n",
    "    dic[patentid[i]]=final_tokens[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the',\n",
       " u'invention',\n",
       " u'relate',\n",
       " u'to',\n",
       " u'a',\n",
       " u'method',\n",
       " u'for',\n",
       " u'produce',\n",
       " u'acrylic',\n",
       " u'acid',\n",
       " u'in',\n",
       " u'one',\n",
       " u'step',\n",
       " u'by',\n",
       " u'an',\n",
       " u'oxydehydration',\n",
       " u'reaction',\n",
       " u'of',\n",
       " u'glycerol',\n",
       " u'in',\n",
       " u'the',\n",
       " u'presence',\n",
       " u'of',\n",
       " u'molecular',\n",
       " u'oxygen',\n",
       " u'the',\n",
       " u'reaction',\n",
       " u'preferably',\n",
       " u'carry',\n",
       " u'out',\n",
       " u'in',\n",
       " u'gaseous',\n",
       " u'phase',\n",
       " u'in',\n",
       " u'the',\n",
       " u'presence',\n",
       " u'of',\n",
       " u'a',\n",
       " u'suitable',\n",
       " u'catalyst']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check with the first abstarct\n",
    "dic['07910771']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let's check how many types we have in the whole corpus and the lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9520 \n",
      "Total number of tokens:  281148 \n",
      "Lexical diversity:  29.5323529412\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(dic.values())) #put all of abstract into a whole list\n",
    "vocab = set(words) #extract unique words\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print \"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4:  Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The reason why Removing stopwords is that if the stopwords are not removed, the bigram would give us some words that does not make any sense, such as \"is that\", \"It is\", which were not what we were looking for. So we need to remove the stopwords before we doing the bigrams.\n",
    "Here, Instead of using the built-in stopword list of NLTK, I use a rich stopword list from textfile. I assume that the stopwords textfile is in the same directory with this assignment jupiter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PS:The reason why I append 2 words in stopwords file below is that after I remove the stopwords(without appending these two) and search in bigram, I notice that 'include' and 'provide' are the most frequent two words that appears as verb, which seems not a bigram collections with other words such as 'include tree'. So I remove them in advance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "#assume stopwords file are under the same directory with this assignment file\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "#append the words that will interfer with searching bigrams\n",
    "stopwords.append('/')\n",
    "stopwords.append('include')\n",
    "stopwords.append('provide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'invention',\n",
       " u'relate',\n",
       " u'method',\n",
       " u'produce',\n",
       " u'acrylic',\n",
       " u'acid',\n",
       " u'step',\n",
       " u'oxydehydration',\n",
       " u'reaction',\n",
       " u'glycerol',\n",
       " u'presence',\n",
       " u'molecular',\n",
       " u'oxygen',\n",
       " u'reaction',\n",
       " u'preferably',\n",
       " u'carry',\n",
       " u'gaseous',\n",
       " u'phase',\n",
       " u'presence',\n",
       " u'suitable',\n",
       " u'catalyst']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list = set(stopwords)\n",
    "for k, v in dic.iteritems():\n",
    "    dic[k] = [word for word in v if word not in stopwords_list]\n",
    "dic['07910771']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let's check how many types we have in the whole corpus and the lexical diversity again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9198 \n",
      "Total number of tokens:  144245 \n",
      "Lexical diversity:  15.6822135247\n"
     ]
    }
   ],
   "source": [
    "#repeat the procedure\n",
    "words_1 = list(chain.from_iterable(dic.values()))\n",
    "vocab_1 = set(words_1)\n",
    "lexical_diversity1 = len(words_1)/len(vocab_1)\n",
    "print \"Vocabulary size: \",len(vocab_1),\"\\nTotal number of tokens: \", len(words_1), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data, we can see that the number of vocabularies decreases about 400 words. And lexical diversity decreases as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5:  Extracting bigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I also use 2 methods to know which bigram collection is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try: Extracting from a text a list of n-gram can be easily accomplished with function ngram(). From the results, we could see that some collation words are not what we want, such as 'system method', 'end portion' etc. So this method is not a good way to find bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = ngrams(words_1, n = 2)\n",
    "fdbigram = FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'present', u'invention'), 230),\n",
       " ((u'system', u'method'), 110),\n",
       " ((u'memory', u'cell'), 98),\n",
       " ((u'semiconductor', u'substrate'), 95),\n",
       " ((u'control', u'unit'), 94),\n",
       " ((u'light', u'source'), 89),\n",
       " ((u'memory', u'device'), 84),\n",
       " ((u'electrically', u'connect'), 84),\n",
       " ((u'invention', u'relate'), 83),\n",
       " ((u'control', u'signal'), 79),\n",
       " ((u'light', u'emit'), 76),\n",
       " ((u'integrate', u'circuit'), 76),\n",
       " ((u'layer', u'form'), 76),\n",
       " ((u'power', u'supply'), 75),\n",
       " ((u'circuit', u'board'), 73),\n",
       " ((u'semiconductor', u'device'), 68),\n",
       " ((u'support', u'member'), 66),\n",
       " ((u'main', u'body'), 64),\n",
       " ((u'image', u'data'), 60),\n",
       " ((u'end', u'portion'), 59),\n",
       " ((u'input', u'signal'), 57),\n",
       " ((u'semiconductor', u'layer'), 57),\n",
       " ((u'electronic', u'device'), 54),\n",
       " ((u'liquid', u'crystal'), 54),\n",
       " ((u'method', u'apparatus'), 53),\n",
       " ((u'upper', u'surface'), 51),\n",
       " ((u'output', u'signal'), 49),\n",
       " ((u'method', u'system'), 47),\n",
       " ((u'configure', u'receive'), 47),\n",
       " ((u'control', u'circuit'), 46),\n",
       " ((u'clock', u'signal'), 46),\n",
       " ((u'distal', u'end'), 45),\n",
       " ((u'side', u'wall'), 43),\n",
       " ((u'thin', u'film'), 42),\n",
       " ((u'method', u'manufacture'), 42),\n",
       " ((u'system', u'comprise'), 41),\n",
       " ((u'gate', u'electrode'), 41),\n",
       " ((u'insulation', u'layer'), 40),\n",
       " ((u'bit', u'line'), 39),\n",
       " ((u'film', u'form'), 39),\n",
       " ((u'display', u'panel'), 38),\n",
       " ((u'signal', u'output'), 38),\n",
       " ((u'device', u'comprise'), 38),\n",
       " ((u'disclose', u'method'), 37),\n",
       " ((u'device', u'method'), 37),\n",
       " ((u'portion', u'form'), 37),\n",
       " ((u'apparatus', u'method'), 37),\n",
       " ((u'fluid', u'communication'), 37),\n",
       " ((u'method', u'comprise'), 37),\n",
       " ((u'apparatus', u'comprise'), 37),\n",
       " ((u'fuel', u'cell'), 37),\n",
       " ((u'longitudinal', u'axis'), 36),\n",
       " ((u'power', u'source'), 36),\n",
       " ((u'frame', u'member'), 36),\n",
       " ((u'user', u'interface'), 35),\n",
       " ((u'embodiment', u'present'), 35),\n",
       " ((u'display', u'device'), 35),\n",
       " ((u'support', u'structure'), 34),\n",
       " ((u'signal', u'control'), 34),\n",
       " ((u'unit', u'configure'), 33),\n",
       " ((u'contact', u'surface'), 33),\n",
       " ((u'print', u'circuit'), 32),\n",
       " ((u'end', u'end'), 32),\n",
       " ((u'laser', u'beam'), 32),\n",
       " ((u'wireless', u'communication'), 32),\n",
       " ((u'image', u'form'), 32),\n",
       " ((u'insulating', u'film'), 31),\n",
       " ((u'conductive', u'layer'), 30),\n",
       " ((u'supply', u'voltage'), 30),\n",
       " ((u'semiconductor', u'die'), 30),\n",
       " ((u'comprise', u'plurality'), 30),\n",
       " ((u'input', u'output'), 30),\n",
       " ((u'control', u'system'), 30),\n",
       " ((u'signal', u'generate'), 30),\n",
       " ((u'surface', u'semiconductor'), 29),\n",
       " ((u'data', u'packet'), 29),\n",
       " ((u'communication', u'system'), 29),\n",
       " ((u'receive', u'data'), 29),\n",
       " ((u'light', u'beam'), 29),\n",
       " ((u'crystal', u'display'), 29),\n",
       " ((u'combustion', u'engine'), 29),\n",
       " ((u'internal', u'combustion'), 29),\n",
       " ((u'form', u'substrate'), 29),\n",
       " ((u'flow', u'passage'), 28),\n",
       " ((u'magnetic', u'layer'), 28),\n",
       " ((u'fluid', u'flow'), 28),\n",
       " ((u'surface', u'substrate'), 28),\n",
       " ((u'processing', u'apparatus'), 28),\n",
       " ((u'comprise', u'step'), 27),\n",
       " ((u'pivotally', u'connect'), 27),\n",
       " ((u'optical', u'axis'), 27),\n",
       " ((u'upper', u'portion'), 27),\n",
       " ((u'electrode', u'form'), 27),\n",
       " ((u'storage', u'device'), 27),\n",
       " ((u'dielectric', u'layer'), 27),\n",
       " ((u'time', u'period'), 27),\n",
       " ((u'reference', u'voltage'), 27),\n",
       " ((u'planetary', u'gear'), 26),\n",
       " ((u'integrated', u'circuit'), 26),\n",
       " ((u'electrical', u'contact'), 26)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 most common bigrams\n",
    "fdbigram.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the most 100 co-orccuring words and ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'0.25', u'gbytes'),\n",
       " (u'047', u'958'),\n",
       " (u'064', u'nm'),\n",
       " (u'1.2', u'picoliters'),\n",
       " (u'1.625', u'1.835'),\n",
       " (u'13', u'mole'),\n",
       " (u'16.5', u'18'),\n",
       " (u'160', u'mg'),\n",
       " (u'18%', u'76%'),\n",
       " (u'26', u'gage'),\n",
       " (u'51', u'76'),\n",
       " (u'52', u'88'),\n",
       " (u'5300', u'6000'),\n",
       " (u'600', u'psig'),\n",
       " (u'65', u'90%'),\n",
       " (u'82%', u'24%'),\n",
       " (u'92', u'efficacy'),\n",
       " (u'adhering', u'compactly'),\n",
       " (u'adopt', u'opened-out'),\n",
       " (u'aliphatic', u'polyol'),\n",
       " (u'allot', u'end-user'),\n",
       " (u'alpine', u'cross-country'),\n",
       " (u'animated', u'gif'),\n",
       " (u'aseptic', u'squeezable'),\n",
       " (u'aspheric', u'inflection'),\n",
       " (u'assembled', u'segregate'),\n",
       " (u'attention', u'peer'),\n",
       " (u'attributed', u'categorical'),\n",
       " (u'autonomic', u'neuropathy'),\n",
       " (u'av', u'synchrony'),\n",
       " (u'bactericide', u'fungicide'),\n",
       " (u'bayonet', u'matable'),\n",
       " (u'bifurcated', u'vasculature'),\n",
       " (u'birefringence', u'0.02'),\n",
       " (u'biscuit', u'65'),\n",
       " (u'borate', u'derivate'),\n",
       " (u'breakage', u'frangible'),\n",
       " (u'broadcasting-terrestrial', u'isdb-t'),\n",
       " (u'broaden', u'sputtered'),\n",
       " (u'buildup', u'sticking'),\n",
       " (u'carboxyl-functional', u'cross-linking'),\n",
       " (u'cardio', u'electrotherapy'),\n",
       " (u'cardiovascular', u'autonomic'),\n",
       " (u'cdc', u'cpsc'),\n",
       " (u'centerers', u'gyroscopic'),\n",
       " (u'chaffer', u'harvester'),\n",
       " (u'channel-lock', u'plier'),\n",
       " (u'chf', u'obesity'),\n",
       " (u'child-resistant', u'fold-over'),\n",
       " (u'chocolate', u'egg')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following code will find the best 100 bigrams using the PMI scores.\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words_1)\n",
    "finder.nbest(bigram_measures.pmi, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Second try: spanning intervening words. It is useful to apply filters, in this case, we ignore all bigrams which have less than three times in the corpus. From this method output, every bigram looks much better. I accept this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'present', u'invention'),\n",
       " (u'memory', u'cell'),\n",
       " (u'invention', u'relate'),\n",
       " (u'light', u'emit'),\n",
       " (u'electrically', u'connect'),\n",
       " (u'integrate', u'circuit'),\n",
       " (u'light', u'source'),\n",
       " (u'liquid', u'crystal'),\n",
       " (u'semiconductor', u'substrate'),\n",
       " (u'circuit', u'board'),\n",
       " (u'power', u'supply'),\n",
       " (u'main', u'body'),\n",
       " (u'thin', u'film'),\n",
       " (u'system', u'method'),\n",
       " (u'planetary', u'gear'),\n",
       " (u'laser', u'beam'),\n",
       " (u'embodiment', u'invention'),\n",
       " (u'combustion', u'engine'),\n",
       " (u'longitudinal', u'axis'),\n",
       " (u'region', u'region'),\n",
       " (u'present', u'relate'),\n",
       " (u'print', u'board'),\n",
       " (u'clock', u'signal'),\n",
       " (u'internal', u'combustion'),\n",
       " (u'method', u'manufacture'),\n",
       " (u'fuel', u'cell'),\n",
       " (u'layer', u'layer'),\n",
       " (u'control', u'unit'),\n",
       " (u'semiconductor', u'device'),\n",
       " (u'permanent', u'magnet'),\n",
       " (u'insulating', u'film'),\n",
       " (u'output', u'signal'),\n",
       " (u'insulation', u'layer'),\n",
       " (u'input', u'output'),\n",
       " (u'memory', u'device'),\n",
       " (u'wind', u'turbine'),\n",
       " (u'side', u'wall'),\n",
       " (u'input', u'signal'),\n",
       " (u'gate', u'electrode'),\n",
       " (u'call', u'party'),\n",
       " (u'display', u'panel'),\n",
       " (u'wireless', u'communication'),\n",
       " (u'electrode', u'electrode'),\n",
       " (u'user', u'interface'),\n",
       " (u'crystal', u'display'),\n",
       " (u'internal', u'engine'),\n",
       " (u'conductivity', u'type'),\n",
       " (u'generate', u'signal'),\n",
       " (u'section', u'section'),\n",
       " (u'layer', u'form'),\n",
       " (u'member', u'member'),\n",
       " (u'flow', u'passage'),\n",
       " (u'support', u'member'),\n",
       " (u'fluid', u'communication'),\n",
       " (u'display', u'display'),\n",
       " (u'resource', u'allocation'),\n",
       " (u'upper', u'surface'),\n",
       " (u'heat', u'sink'),\n",
       " (u'electronic', u'device'),\n",
       " (u'portion', u'portion'),\n",
       " (u'terminal', u'terminal'),\n",
       " (u'memory', u'array'),\n",
       " (u'emit', u'diode'),\n",
       " (u'source', u'drain'),\n",
       " (u'dielectric', u'layer'),\n",
       " (u'non-volatile', u'memory'),\n",
       " (u'golf', u'club'),\n",
       " (u'heat', u'exchanger'),\n",
       " (u'hydrophobic', u'drug'),\n",
       " (u'image', u'data'),\n",
       " (u'embodiment', u'present'),\n",
       " (u'semiconductor', u'layer'),\n",
       " (u'control', u'signal'),\n",
       " (u'hard', u'mask'),\n",
       " (u'magnetic', u'field'),\n",
       " (u'time', u'period'),\n",
       " (u'control', u'control'),\n",
       " (u'integrated', u'circuit'),\n",
       " (u'preferred', u'embodiment'),\n",
       " (u'insulative', u'housing'),\n",
       " (u'cell', u'array'),\n",
       " (u'port', u'port'),\n",
       " (u'rfid', u'reader'),\n",
       " (u'print', u'circuit'),\n",
       " (u'printed', u'board'),\n",
       " (u'power', u'source'),\n",
       " (u'open', u'close'),\n",
       " (u'configure', u'receive'),\n",
       " (u'electric', u'motor'),\n",
       " (u'reference', u'voltage'),\n",
       " (u'hydraulic', u'pressure'),\n",
       " (u'pivotally', u'connect'),\n",
       " (u'signal', u'output'),\n",
       " (u'accelerator', u'pedal'),\n",
       " (u'fluid', u'flow'),\n",
       " (u'disclose', u'method'),\n",
       " (u'form', u'substrate'),\n",
       " (u'image', u'image'),\n",
       " (u'hardware', u'node'),\n",
       " (u'computer', u'program')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "#use wondow size used for the collocations extraction\n",
    "finder = BigramCollocationFinder.from_words(words_1,window_size=3)\n",
    "#use filter to ignore all bigrams which occur less than three times\n",
    "finder.apply_freq_filter(3)\n",
    "finder.apply_word_filter(lambda w: len(w) < 4 or w in stopwords)\n",
    "filtered_words=finder.nbest(bigram_measures.likelihood_ratio, 100)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'present', u'invention'), 231),\n",
       " ((u'system', u'method'), 127),\n",
       " ((u'portion', u'portion'), 122),\n",
       " ((u'control', u'unit'), 98),\n",
       " ((u'memory', u'cell'), 98),\n",
       " ((u'semiconductor', u'substrate'), 97),\n",
       " ((u'layer', u'layer'), 95),\n",
       " ((u'light', u'source'), 95),\n",
       " ((u'layer', u'form'), 92),\n",
       " ((u'semiconductor', u'device'), 89),\n",
       " ((u'control', u'signal'), 88),\n",
       " ((u'electrically', u'connect'), 87),\n",
       " ((u'member', u'member'), 87),\n",
       " ((u'signal', u'signal'), 86),\n",
       " ((u'memory', u'device'), 84),\n",
       " ((u'invention', u'relate'), 83),\n",
       " ((u'light', u'emit'), 83),\n",
       " ((u'output', u'signal'), 82),\n",
       " ((u'control', u'control'), 80),\n",
       " ((u'power', u'supply'), 80),\n",
       " ((u'integrate', u'circuit'), 77),\n",
       " ((u'circuit', u'board'), 73),\n",
       " ((u'generate', u'signal'), 72),\n",
       " ((u'support', u'member'), 71),\n",
       " ((u'input', u'signal'), 69),\n",
       " ((u'image', u'data'), 68),\n",
       " ((u'region', u'region'), 67),\n",
       " ((u'surface', u'surface'), 66),\n",
       " ((u'form', u'substrate'), 64),\n",
       " ((u'main', u'body'), 64),\n",
       " ((u'position', u'position'), 64),\n",
       " ((u'signal', u'output'), 63),\n",
       " ((u'device', u'device'), 60),\n",
       " ((u'method', u'apparatus'), 60),\n",
       " ((u'semiconductor', u'layer'), 60),\n",
       " ((u'unit', u'unit'), 60),\n",
       " ((u'method', u'system'), 56),\n",
       " ((u'receive', u'signal'), 56),\n",
       " ((u'electronic', u'device'), 55),\n",
       " ((u'embodiment', u'invention'), 55),\n",
       " ((u'input', u'output'), 55),\n",
       " ((u'upper', u'surface'), 55),\n",
       " ((u'data', u'data'), 54),\n",
       " ((u'device', u'method'), 54),\n",
       " ((u'liquid', u'crystal'), 54),\n",
       " ((u'contact', u'surface'), 53),\n",
       " ((u'form', u'layer'), 53),\n",
       " ((u'form', u'surface'), 53),\n",
       " ((u'surface', u'substrate'), 52),\n",
       " ((u'system', u'comprise'), 51),\n",
       " ((u'clock', u'signal'), 50),\n",
       " ((u'control', u'circuit'), 50),\n",
       " ((u'section', u'section'), 50),\n",
       " ((u'configure', u'receive'), 49),\n",
       " ((u'electrode', u'electrode'), 49),\n",
       " ((u'image', u'image'), 49),\n",
       " ((u'side', u'wall'), 49),\n",
       " ((u'signal', u'control'), 49),\n",
       " ((u'disclose', u'method'), 48),\n",
       " ((u'invention', u'method'), 47),\n",
       " ((u'apparatus', u'method'), 46),\n",
       " ((u'connect', u'portion'), 46),\n",
       " ((u'display', u'display'), 46),\n",
       " ((u'receive', u'data'), 46),\n",
       " ((u'device', u'comprise'), 45),\n",
       " ((u'film', u'form'), 45),\n",
       " ((u'method', u'manufacture'), 45),\n",
       " ((u'portion', u'form'), 45),\n",
       " ((u'signal', u'unit'), 45),\n",
       " ((u'layer', u'substrate'), 44),\n",
       " ((u'store', u'data'), 43),\n",
       " ((u'thin', u'film'), 43),\n",
       " ((u'display', u'panel'), 42),\n",
       " ((u'present', u'relate'), 42),\n",
       " ((u'circuit', u'circuit'), 41),\n",
       " ((u'fluid', u'communication'), 41),\n",
       " ((u'gate', u'electrode'), 41),\n",
       " ((u'housing', u'housing'), 41),\n",
       " ((u'power', u'source'), 41),\n",
       " ((u'apparatus', u'comprise'), 40),\n",
       " ((u'fuel', u'cell'), 40),\n",
       " ((u'insulation', u'layer'), 40),\n",
       " ((u'terminal', u'terminal'), 40),\n",
       " ((u'form', u'semiconductor'), 39),\n",
       " ((u'image', u'apparatus'), 39),\n",
       " ((u'outer', u'surface'), 39),\n",
       " ((u'signal', u'generate'), 39),\n",
       " ((u'unit', u'control'), 39),\n",
       " ((u'display', u'device'), 38),\n",
       " ((u'frame', u'member'), 38),\n",
       " ((u'method', u'comprise'), 38),\n",
       " ((u'support', u'support'), 38),\n",
       " ((u'conductive', u'layer'), 37),\n",
       " ((u'control', u'valve'), 37),\n",
       " ((u'electrode', u'form'), 37),\n",
       " ((u'longitudinal', u'axis'), 37),\n",
       " ((u'portion', u'connect'), 37),\n",
       " ((u'surface', u'semiconductor'), 37),\n",
       " ((u'comprise', u'plurality'), 36),\n",
       " ((u'control', u'system'), 36)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the frequency of the bigrams which I searched from last step\n",
    "sorted(finder.ngram_fd.items(), key=lambda t: (-t[1], t[0]))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'invention_relate',\n",
       " u'method',\n",
       " u'produce',\n",
       " u'acrylic',\n",
       " u'acid',\n",
       " u'step',\n",
       " u'oxydehydration',\n",
       " u'reaction',\n",
       " u'glycerol',\n",
       " u'presence',\n",
       " u'molecular',\n",
       " u'oxygen',\n",
       " u'reaction',\n",
       " u'preferably',\n",
       " u'carry',\n",
       " u'gaseous',\n",
       " u'phase',\n",
       " u'presence',\n",
       " u'suitable',\n",
       " u'catalyst']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace bigram with each corresponding words in abstracts\n",
    "mwe_tokenizer = MWETokenizer()\n",
    "for key,value in dic.iteritems():\n",
    "    pre_value=list(set(dic[key]))\n",
    "    for each in filtered_words:\n",
    "        #append bigrams to each abstracts\n",
    "        pre_value.append(each)\n",
    "    #replace bigrams with original words\n",
    "    mwe_tokenizer = MWETokenizer(pre_value)\n",
    "    dic[key]=mwe_tokenizer.tokenize(dic[key])\n",
    "#check the first abstract\n",
    "dic['07910771']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6:  Removing top-20 frequency words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I think the reason why we need to remove frequency words is that if a word appears in several abstracts, it would interfere with search relevance data. For example, if I search a patent, system may found a lot of patents relevant to that patent because they have a lot of same words. In this case, user may not find what they turely interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEyCAYAAAAyQk1tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEEQBL3xbxxgyEspDY/EAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2212ae80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_2 = list(chain.from_iterable(dic.values()))\n",
    "#use freqdist to find the corresponding frequcy with each word\n",
    "fd_1 = FreqDist(words_2)\n",
    "#show the words\n",
    "vocabulary1=fd_1.keys()\n",
    "#show the top-20 frequency words\n",
    "frequent_words=vocabulary1[:20]\n",
    "#make a plot to visulazation\n",
    "fd_1.plot(25, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'invention_relate',\n",
       " u'method',\n",
       " u'produce',\n",
       " u'acrylic',\n",
       " u'acid',\n",
       " u'step',\n",
       " u'oxydehydration',\n",
       " u'reaction',\n",
       " u'glycerol',\n",
       " u'presence',\n",
       " u'molecular',\n",
       " u'oxygen',\n",
       " u'reaction',\n",
       " u'preferably',\n",
       " u'carry',\n",
       " u'gaseous',\n",
       " u'phase',\n",
       " u'presence',\n",
       " u'suitable',\n",
       " u'catalyst']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove top-20 words for each value in dictionary\n",
    "for k, v in dic.iteritems():\n",
    "    dic[k] = [word for word in v if word not in frequent_words]\n",
    "dic['07910771']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 7:  Removing words that only apperaring once in all abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I try to use hapaxes() function to count the words that only appear once in the whole text. But it seems that it does not work for some words that appear more than once under an Abstract of a Patent ID and also does not appear in the other abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not recommeneded\n",
    "#find the words which only appear once in all abstracts\n",
    "lessFreqWords = set(fd_1.hapaxes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3277"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lessFreqWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I try another way to find the words appearing only once and the words which appears more than once in a single abstracts but does not appear under the other abstrcts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_list=[]\n",
    "for v in dic.values():\n",
    "    value_list.append(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I write following code by myself to look for those satisified words and avoid removing bigrams simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Running this program may take a few munites, thanks for waiting\n",
    "one_abstract_word=[]\n",
    "temp_list_use=[]\n",
    "#finding the words that does not appear in other abstracts\n",
    "for i in range(len(value_list)):\n",
    "    #if index=0, put abstract[1:] into a list, finding any words in first index doesn't appear in anther list\n",
    "    if i==0:\n",
    "        new_list=value_list[1:]\n",
    "        for each in new_list:\n",
    "            temp_list_use+=each\n",
    "        for each in value_list[0]:\n",
    "            if each not in temp_list_use:\n",
    "                one_abstract_word.append(each)\n",
    "        temp_list_use=[]\n",
    "        i+=1\n",
    "    #if i=last abstracts, put abstrcts[：-1] togther and make a list. Finding if any words in last abstract appears in former list.\n",
    "    elif i==len(value_list)-1:\n",
    "        new_list=value_list[:i]\n",
    "        for each in new_list:\n",
    "            temp_list_use+=each\n",
    "        for each in value_list[i]:\n",
    "            if each not in temp_list_use:\n",
    "                one_abstract_word.append(each)\n",
    "        temp_list_use=[]\n",
    "        i+=1\n",
    "    #make the bastract before and after index togther as a list. Finding if the words in index list appepars in another combined list.\n",
    "    else:\n",
    "        new_list=value_list[:i]+value_list[i+1:]\n",
    "        for item in new_list:\n",
    "            temp_list_use+=item\n",
    "        for each in value_list[i]:\n",
    "            if each not in temp_list_use:\n",
    "                one_abstract_word.append(each)\n",
    "        temp_list_use=[]\n",
    "        i+=1\n",
    "\n",
    "one_abstract_word_list=list(set(one_abstract_word))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print list from last step\n",
    "#one_abstract_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following below shows the number of words I found from my code, which is more than about 1000 words need to be removed than using hapaxes() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4783"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_abstract_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now removing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in dic.iteritems():\n",
    "    dic[k] = [word for word in v if word not in one_abstract_word_list]\n",
    "words_3=list(chain.from_iterable(dic.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  4486 \n",
      "Total number of tokens:  132808 \n",
      "Lexical diversity:  29.6049933125\n"
     ]
    }
   ],
   "source": [
    "vocab_3 = set(words_3)\n",
    "lexical_diversity3 = len(words_3)/len(vocab_3)\n",
    "print \"Vocabulary size: \",len(vocab_3),\"\\nTotal number of tokens: \", len(words_3), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the 'vocab' file, I found some numbers that does not help to search the relevant data. \n",
    "(This step should have done after building a vocab file. But in order to make the procedure clean and clear, I add this step before the final build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove numbers\n",
    "for k, v in dic.iteritems():\n",
    "    dic[k] = [word for word in v if not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  4453 \n",
      "Total number of tokens:  132529 \n",
      "Lexical diversity:  29.7617336627\n"
     ]
    }
   ],
   "source": [
    "words_4=list(chain.from_iterable(dic.values()))\n",
    "vocab_4 = set(words_4)\n",
    "lexical_diversity4 = len(words_4)/len(vocab_4)\n",
    "print \"Vocabulary size: \",len(vocab_4),\"\\nTotal number of tokens: \", len(words_4), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the PatentID and its abstract/abstracts to a text file. This step should be done twice. The first time is to use vocab_3 to build a file. After reviewing the file, I found some words look strange and can not help with further uses. Therefore, after removing those words, I build it at the second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_5=list(set(chain.from_iterable(dic.values())))\n",
    "output_handle=open('vocab.txt','w+')\n",
    "\n",
    "for num in range(len(words_5)):\n",
    "    line=str(num)+str(\":\")+str(words_5[num].encode('utf-8'))+\"\\n\"\n",
    "    output_handle.write(line)\n",
    "\n",
    "output_handle.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### STEP 8: Build count vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, I generate a count vector represented for each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \"fit_transform\" to fit the model and learn from the vocabulary. And then transform the text data into feature vectors. Since dictionary stored each tokenised article in a list, we concatenate all of the words in a list and separate them with the white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 4335)\n"
     ]
    }
   ],
   "source": [
    "data_features = vectorizer.fit_transform([' '.join(value) for value in dic.values()])\n",
    "print data_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save such a matrix in our computer memory but also speed up algebraic operations on a matrix, scikit-learn implements matrix/vector in a sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecg : 1\n",
      "direct : 1\n",
      "mating : 1\n",
      "gauge : 1\n",
      "stain : 1\n",
      "doped : 1\n",
      "operator : 1\n",
      "select : 1\n",
      "radiotherapy : 1\n",
      "wager : 1\n",
      "diametrically : 1\n",
      "improved : 1\n",
      "sink : 2\n",
      "radiofrequency : 1\n",
      "couple : 2\n",
      "dielectric_layer : 1\n",
      "1.5 : 1\n"
     ]
    }
   ],
   "source": [
    "vocab2 = vectorizer.get_feature_names()\n",
    "for word, count in zip(words_5, data_features.toarray()[0]):\n",
    "    if count > 0:\n",
    "        print word, \":\", count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to get the count list by using FreqDist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({u'acid': 1,\n",
       "          u'acrylic': 1,\n",
       "          u'carry': 1,\n",
       "          u'catalyst': 1,\n",
       "          u'gaseous': 1,\n",
       "          u'glycerol': 1,\n",
       "          u'invention_relate': 1,\n",
       "          u'method': 1,\n",
       "          u'molecular': 1,\n",
       "          u'oxygen': 1,\n",
       "          u'phase': 1,\n",
       "          u'preferably': 1,\n",
       "          u'presence': 2,\n",
       "          u'produce': 1,\n",
       "          u'reaction': 2,\n",
       "          u'step': 1,\n",
       "          u'suitable': 1})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(dic['07910771'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: Writing a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_file = open('count_vector.txt', 'w') #invention relate method\n",
    "vocab = words_5\n",
    "line=\"\"\n",
    "for g,d in dic.iteritems():\n",
    "    out_file.write(g)\n",
    "    #add comma before patentid\n",
    "    out_file.write(\",\")\n",
    "    d_idx = [vocab.index(w) for w in d]\n",
    "    for k, v in FreqDist(d_idx).iteritems():\n",
    "        #formate with words and index\n",
    "        a=\"{}:{}\".format(k,v)\n",
    "        line+=str(a)+str(\",\")\n",
    "    #remove comma at each end of the line\n",
    "    new_line=line.rstrip(\",\")\n",
    "    out_file.write(new_line)\n",
    "    line=\"\"\n",
    "    out_file.write('\\n')\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Summay:\n",
    "From this project, we pre-process several elements of the patents from the XML file. \n",
    "* 1.Extract and write a text file with the elements of: patent’s_ID:Section,Class,Subclass,Main_group,Subgroup.\n",
    "* 2.Extract each patent's citation and write a file with the format of: citing_patent_id:cited_patent_id,cited_patent_id...\n",
    "* 3.Count how many times that a particular patent has been cited and then save them in a file with format:cited_patent_id: times.\n",
    "* 4.Extract all of the abstracts of all of the patents, and then process and store those abstracts as a sparse count vectors format. Then generate a vocabulary file which stores vocabularies and its indexes in a count_vectors file for the future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
